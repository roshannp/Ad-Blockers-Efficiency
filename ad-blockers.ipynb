{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Selenium + BrowserMob Proxy to Record HAR Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from browsermobproxy import Server\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Start BrowserMob Proxy\n",
    "server = Server(\"path/to/browsermob-proxy\")  # Download from https://bmp.lightbody.net/\n",
    "server.start()\n",
    "proxy = server.create_proxy()\n",
    "\n",
    "# Set Chrome options to use proxy\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"--proxy-server={proxy.proxy}\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# List of websites to test\n",
    "websites = [\n",
    "    \"https://www.cnn.com\",\n",
    "    \"https://www.apple.com\",\n",
    "    \"https://www.amazon.com\",\n",
    "    # ... up to 100 sites\n",
    "]\n",
    "\n",
    "for site in websites:\n",
    "    proxy.new_har(site, options={'captureHeaders': True, 'captureContent': True})\n",
    "    driver.get(site)\n",
    "    time.sleep(5)  # Wait for full load\n",
    "\n",
    "    # Save HAR file\n",
    "    har_data = proxy.har\n",
    "    with open(f\"har_files/{site.replace('https://', '').replace('/', '_')}.har\", \"w\") as f:\n",
    "        json.dump(har_data, f)\n",
    "\n",
    "# Cleanup\n",
    "driver.quit()\n",
    "proxy.stop()\n",
    "server.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAR Parser in Python to Extract Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def parse_har_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        har = json.load(f)\n",
    "\n",
    "    third_party_requests = set()\n",
    "    domains = set()\n",
    "    durations = []\n",
    "    total_time = 0\n",
    "\n",
    "    for entry in har['log']['entries']:\n",
    "        url = entry['request']['url']\n",
    "        domain = url.split('/')[2] if '://' in url else url\n",
    "        if not domain.endswith(\"cnn.com\"):  # Replace with main domain\n",
    "            third_party_requests.add(url)\n",
    "            domains.add(domain)\n",
    "\n",
    "        durations.append(entry['time'])\n",
    "        total_time += entry['time']\n",
    "\n",
    "    metrics = {\n",
    "        \"third_party_requests\": len(third_party_requests),\n",
    "        \"unique_domains\": len(domains),\n",
    "        \"avg_duration\": total_time / len(durations),\n",
    "        \"95_percentile\": sorted(durations)[int(0.95 * len(durations)) - 1]\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Loop over all HAR files\n",
    "har_dir = \"har_files\"\n",
    "results = []\n",
    "for filename in os.listdir(har_dir):\n",
    "    filepath = os.path.join(har_dir, filename)\n",
    "    metrics = parse_har_file(filepath)\n",
    "    results.append(metrics)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize Metrics Per Blocker and Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"Average 3rd-party Requests Blocked:\", df['third_party_requests'].mean())\n",
    "print(\"Average Unique Domains:\", df['unique_domains'].mean())\n",
    "print(\"Average Request Duration:\", df['avg_duration'].mean())\n",
    "print(\"95th Percentile Duration:\", df['95_percentile'].mean())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
